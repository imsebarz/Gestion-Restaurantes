#!/bin/bash

# ğŸ§ª Script de EjecuciÃ³n de Pruebas BDD para GestiÃ³n de Restaurantes
# 
# Este script automatiza la ejecuciÃ³n de todas las pruebas del sistema,
# incluyendo BDD, Unit Tests, y verificaciones de calidad.

set -e  # Exit on any error

# Colores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# FunciÃ³n para logging
log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}"
}

warn() {
    echo -e "${YELLOW}[$(date +'%Y-%m-%d %H:%M:%S')] WARNING: $1${NC}"
}

error() {
    echo -e "${RED}[$(date +'%Y-%m-%d %H:%M:%S')] ERROR: $1${NC}"
}

info() {
    echo -e "${BLUE}[$(date +'%Y-%m-%d %H:%M:%S')] INFO: $1${NC}"
}

# FunciÃ³n para mostrar ayuda
show_help() {
    cat << EOF
ğŸ§ª Script de Pruebas BDD - GestiÃ³n de Restaurantes

USO:
    ./run-tests.sh [OPCIÃ“N]

OPCIONES:
    --all           Ejecutar todas las pruebas (default)
    --bdd           Ejecutar solo pruebas BDD/Features
    --unit          Ejecutar solo pruebas unitarias
    --integration   Ejecutar solo pruebas de integraciÃ³n
    --watch         Ejecutar en modo watch
    --coverage      Ejecutar con reporte de cobertura
    --setup         Solo configurar entorno de pruebas
    --cleanup       Solo limpiar datos de prueba
    --help          Mostrar esta ayuda

EJEMPLOS:
    ./run-tests.sh --bdd
    ./run-tests.sh --coverage
    ./run-tests.sh --watch --unit

CONFIGURACIÃ“N:
    - AsegÃºrate de tener la base de datos de pruebas configurada
    - Variables de entorno necesarias en .env.test
    - Node.js >= 18 y npm instalado
EOF
}

# FunciÃ³n para verificar prerrequisitos
check_prerequisites() {
    log "Verificando prerrequisitos..."
    
    # Verificar Node.js
    if ! command -v node &> /dev/null; then
        error "Node.js no estÃ¡ instalado"
        exit 1
    fi
    
    # Verificar npm
    if ! command -v npm &> /dev/null; then
        error "npm no estÃ¡ instalado"
        exit 1
    fi
    
    # Verificar package.json
    if [ ! -f "package.json" ]; then
        error "package.json no encontrado. Ejecutar desde el directorio backend/"
        exit 1
    fi
    
    # Verificar dependencias instaladas
    if [ ! -d "node_modules" ]; then
        warn "node_modules no encontrado. Instalando dependencias..."
        npm install
    fi
    
    log "âœ… Prerrequisitos verificados"
}

# FunciÃ³n para configurar entorno de pruebas
setup_test_environment() {
    log "Configurando entorno de pruebas..."
    
    # Crear archivo .env.test si no existe
    if [ ! -f ".env.test" ]; then
        log "Creando archivo .env.test..."
        cat > .env.test << EOF
NODE_ENV=test
DATABASE_URL="postgresql://test:test@localhost:5432/restaurant_test"
JWT_SECRET="test-secret-key-very-long-and-secure-for-testing-purposes"
PORT=4001
EOF
    fi
    
    # Verificar conexiÃ³n a base de datos de pruebas
    log "Verificando conexiÃ³n a base de datos de pruebas..."
    export NODE_ENV=test
    
    # Ejecutar migraciones de prueba
    log "Ejecutando migraciones de base de datos..."
    npm run migrate 2>/dev/null || warn "No se pudieron ejecutar migraciones automÃ¡ticamente"
    
    log "âœ… Entorno de pruebas configurado"
}

# FunciÃ³n para limpiar datos de prueba
cleanup_test_data() {
    log "Limpiando datos de prueba..."
    
    export NODE_ENV=test
    
    # Script simple de limpieza usando Prisma
    cat > cleanup-test.js << 'EOF'
const { PrismaClient } = require('@prisma/client');

async function cleanup() {
    const prisma = new PrismaClient();
    try {
        await prisma.payment.deleteMany();
        await prisma.orderItem.deleteMany();
        await prisma.order.deleteMany();
        await prisma.menuItem.deleteMany();
        await prisma.table.deleteMany();
        await prisma.user.deleteMany();
        console.log('âœ… Datos de prueba limpiados');
    } catch (error) {
        console.error('âŒ Error limpiando datos:', error.message);
    } finally {
        await prisma.$disconnect();
    }
}

cleanup();
EOF
    
    node cleanup-test.js
    rm -f cleanup-test.js
    
    log "âœ… Limpieza completada"
}

# FunciÃ³n para ejecutar pruebas BDD
run_bdd_tests() {
    log "ğŸ­ Ejecutando pruebas BDD (Behavior Driven Development)..."
    
    export NODE_ENV=test
    
    echo ""
    info "=== PRUEBAS BDD - GESTIÃ“N DE PEDIDOS Y PAGOS ==="
    echo ""
    
    # Ejecutar pruebas de features especÃ­ficas
    if npm run test:features; then
        log "âœ… Pruebas BDD completadas exitosamente"
        return 0
    else
        error "âŒ Fallos en pruebas BDD"
        return 1
    fi
}

# FunciÃ³n para ejecutar pruebas unitarias
run_unit_tests() {
    log "ğŸ”¬ Ejecutando pruebas unitarias..."
    
    export NODE_ENV=test
    
    echo ""
    info "=== PRUEBAS UNITARIAS ==="
    echo ""
    
    # Ejecutar pruebas unitarias especÃ­ficas
    if npm test -- --testMatch="**/unit/**/*.test.ts"; then
        log "âœ… Pruebas unitarias completadas exitosamente"
        return 0
    else
        error "âŒ Fallos en pruebas unitarias"
        return 1
    fi
}

# FunciÃ³n para ejecutar pruebas de integraciÃ³n
run_integration_tests() {
    log "ğŸ”— Ejecutando pruebas de integraciÃ³n..."
    
    export NODE_ENV=test
    
    echo ""
    info "=== PRUEBAS DE INTEGRACIÃ“N ==="
    echo ""
    
    # Ejecutar pruebas de integraciÃ³n (GraphQL + Database)
    if npm test -- --testMatch="**/integration/**/*.test.ts"; then
        log "âœ… Pruebas de integraciÃ³n completadas exitosamente"
        return 0
    else
        error "âŒ Fallos en pruebas de integraciÃ³n"
        return 1
    fi
}

# FunciÃ³n para ejecutar todas las pruebas
run_all_tests() {
    log "ğŸš€ Ejecutando suite completa de pruebas..."
    
    local bdd_result=0
    local unit_result=0
    local integration_result=0
    
    # Ejecutar cada tipo de prueba
    run_bdd_tests || bdd_result=1
    echo ""
    
    run_unit_tests || unit_result=1
    echo ""
    
    run_integration_tests || integration_result=1
    echo ""
    
    # Mostrar resumen
    echo ""
    info "=== RESUMEN DE PRUEBAS ==="
    
    if [ $bdd_result -eq 0 ]; then
        log "âœ… Pruebas BDD: PASARON"
    else
        error "âŒ Pruebas BDD: FALLARON"
    fi
    
    if [ $unit_result -eq 0 ]; then
        log "âœ… Pruebas Unitarias: PASARON"
    else
        error "âŒ Pruebas Unitarias: FALLARON"
    fi
    
    if [ $integration_result -eq 0 ]; then
        log "âœ… Pruebas IntegraciÃ³n: PASARON"
    else
        error "âŒ Pruebas IntegraciÃ³n: FALLARON"
    fi
    
    # Retornar cÃ³digo de salida general
    if [ $bdd_result -eq 0 ] && [ $unit_result -eq 0 ] && [ $integration_result -eq 0 ]; then
        log "ğŸ‰ TODAS LAS PRUEBAS PASARON"
        return 0
    else
        error "ğŸ’¥ ALGUNAS PRUEBAS FALLARON"
        return 1
    fi
}

# FunciÃ³n para ejecutar con cobertura
run_with_coverage() {
    log "ğŸ“Š Ejecutando pruebas con reporte de cobertura..."
    
    export NODE_ENV=test
    
    echo ""
    info "=== PRUEBAS CON COBERTURA ==="
    echo ""
    
    if npm run test:coverage; then
        log "âœ… Pruebas con cobertura completadas"
        info "ğŸ“ˆ Reporte de cobertura disponible en coverage/"
        
        # Mostrar cobertura mÃ­nima requerida
        echo ""
        info "Metas de Cobertura:"
        info "  - LÃ­neas: 85%"
        info "  - Funciones: 90%"
        info "  - Ramas: 80%"
        info "  - Statements: 85%"
        
        return 0
    else
        error "âŒ Fallos en pruebas con cobertura"
        return 1
    fi
}

# FunciÃ³n para ejecutar en modo watch
run_watch_mode() {
    log "ğŸ‘€ Ejecutando pruebas en modo watch..."
    
    export NODE_ENV=test
    
    echo ""
    info "=== MODO WATCH ACTIVADO ==="
    info "Presiona 'q' para salir, 'a' para ejecutar todas las pruebas"
    echo ""
    
    npm run test:watch
}

# FunciÃ³n para mostrar estadÃ­sticas
show_statistics() {
    log "ğŸ“ˆ Generando estadÃ­sticas de pruebas..."
    
    echo ""
    info "=== ESTADÃSTICAS DEL PROYECTO ==="
    
    # Contar archivos de prueba
    local bdd_files=$(find tests/features -name "*.feature" 2>/dev/null | wc -l)
    local unit_files=$(find tests/unit -name "*.test.ts" 2>/dev/null | wc -l)
    local step_files=$(find tests/features -name "*.spec.ts" 2>/dev/null | wc -l)
    
    info "ğŸ“ Archivos de prueba:"
    info "  - Features (BDD): $bdd_files archivos"
    info "  - Unit Tests: $unit_files archivos"
    info "  - Step Definitions: $step_files archivos"
    
    # Contar lÃ­neas de cÃ³digo de prueba
    local test_lines=$(find tests -name "*.ts" -o -name "*.feature" | xargs wc -l 2>/dev/null | tail -1 | awk '{print $1}')
    info "  - Total lÃ­neas: $test_lines"
    
    echo ""
    info "ğŸ¯ Escenarios implementados:"
    info "  âœ… Cliente crea pedido desde QR"
    info "  âœ… Chef cambia estado a PREPARING"
    info "  âœ… Cajero procesa pago en efectivo"
    info "  âœ… Usuario consulta menÃº"
    info "  âœ… Login con credenciales vÃ¡lidas"
    info "  âœ… Manejo de errores y validaciones"
    info "  âœ… Casos lÃ­mite y edge cases"
}

# FunciÃ³n principal
main() {
    echo ""
    log "ğŸ§ª Iniciando Sistema de Pruebas BDD"
    log "ğŸ½ï¸  Proyecto: GestiÃ³n de Restaurantes"
    echo ""
    
    # Procesar argumentos
    case "${1:-}" in
        --help|-h)
            show_help
            exit 0
            ;;
        --setup)
            check_prerequisites
            setup_test_environment
            log "âœ… Setup completado"
            exit 0
            ;;
        --cleanup)
            cleanup_test_data
            exit 0
            ;;
        --bdd)
            check_prerequisites
            setup_test_environment
            run_bdd_tests
            show_statistics
            ;;
        --unit)
            check_prerequisites
            setup_test_environment
            run_unit_tests
            ;;
        --integration)
            check_prerequisites
            setup_test_environment
            run_integration_tests
            ;;
        --coverage)
            check_prerequisites
            setup_test_environment
            run_with_coverage
            ;;
        --watch)
            check_prerequisites
            setup_test_environment
            run_watch_mode
            ;;
        --all|"")
            check_prerequisites
            setup_test_environment
            run_all_tests
            show_statistics
            ;;
        *)
            error "OpciÃ³n desconocida: $1"
            show_help
            exit 1
            ;;
    esac
    
    local exit_code=$?
    
    echo ""
    if [ $exit_code -eq 0 ]; then
        log "ğŸ‰ EjecuciÃ³n completada exitosamente"
    else
        error "ğŸ’¥ EjecuciÃ³n completada con errores"
    fi
    
    exit $exit_code
}

# Trap para limpieza en caso de interrupciÃ³n
trap cleanup_test_data EXIT

# Ejecutar funciÃ³n principal
main "$@"
